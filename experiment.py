# -*- coding: utf-8 -*-
"""experiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x0dqbKgDuqZNVVzXrGbBnsqQWNXBfzBR
"""

from google.colab import drive
drive.mount('/content/drive')

from keras import utils
#from Preprocessing import *
#from vggnet import *

# data = simple_process()
# input_data = []
# for i in data:
#     input_data.append(np.resize(i,(128,128)))
# train_y = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/train_max_y.csv')
# train_y[['Label']]

# from sklearn.model_selection import train_test_split
# new_x = np.asarray(input_data).reshape(50000,128,128,1)
# new_y = utils.to_categorical(train_y[['Label']][:50000], 10)
# x_train, x_val, y_train, y_val = train_test_split(new_x, new_y, test_size=0.2)

import pandas as pd
import numpy as np
train_images = pd.read_pickle('/content/drive/My Drive/Colab Notebooks/data/train_max_x')
train_labels = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/train_max_y.csv').to_numpy()[:,1]

from sklearn.model_selection import train_test_split
x_train, x_val,y_train,y_val = train_test_split(train_images,train_labels,test_size=0.2,random_state=42,stratify=train_labels)
x_train = x_train.reshape(x_train.shape[0],128,128,1)
x_val = x_val.reshape(x_val.shape[0],128,128,1)
x_train = x_train.astype('float32')
x_val = x_val.astype('float32')
x_train /= 255
x_val /= 255

y_train = utils.to_categorical(y_train,10)
y_val = utils.to_categorical(y_val,10)

import keras
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Flatten, Concatenate, Input, concatenate, Lambda
from keras.layers.normalization import BatchNormalization
from keras.wrappers.scikit_learn import KerasClassifier
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D
from keras.layers.advanced_activations import LeakyReLU
from keras.optimizers import Adam, RMSprop, Adagrad, Nadam
from math import exp
import warnings
import math
import random
import itertools
# !pip install sklearn
from sklearn.model_selection import train_test_split

from keras import backend as K
from keras.layers import Layer
from keras import activations
from keras import utils
from keras.models import Model
from keras.layers import *
from keras.preprocessing.image import ImageDataGenerator
from keras.regularizers import l2

from keras.models import model_from_json
from keras.datasets import mnist
from keras.utils import np_utils
from keras.models import Sequential, Model
from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten, Input, Dropout
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.callbacks import ReduceLROnPlateau

from sklearn import model_selection
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import LinearSVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Reshape
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import LearningRateScheduler
from keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

import sys
import numpy as np
import cv2
import sklearn.metrics as sklm

from keras.applications.vgg16 import VGG16
from keras.preprocessing import image
from keras.applications.vgg16 import preprocess_input
from keras.layers import Input, Flatten, Dense
from keras.models import Model, load_model
from keras.datasets import mnist

from keras import backend as K

input_shape = (128,128,1)  # change this
batch_size = 128
epochs = 100
num_classes = 10

def lr_schedule(epoch):
    lr = 1e-3
    if epoch > 20:
        lr *= 1e-3
    elif epoch > 15:
        lr *= 1e-2
    elif epoch > 10:
        lr *= 1e-1
    print('Learning rate: ', lr)
    return lr



# VGG-16 modified
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))
model.add(BatchNormalization())
model.add(Conv2D(32, (3, 3),activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
 
model.add(Conv2D(64, kernel_size=(3, 3),activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(64, kernel_size=(3, 3),activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
 
model.add(Conv2D(128, kernel_size=(3, 3),activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(128, kernel_size=(3, 3),activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
 
model.add(Conv2D(128, kernel_size=(3, 3),activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(128, kernel_size=(3, 3),activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.4))
 
model.add(Flatten())
model.add(Dense(1024,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10,activation='softmax'))


# # LeNet
# model = Sequential()
# model.add(Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=input_shape, padding="same"))
# model.add(AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))
# model.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'))
# model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))
# model.add(Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'))
# model.add(Flatten())
# model.add(Dense(84, activation='relu'))
# model.add(Dropout(0.2))
# model.add(Dense(num_classes, activation='softmax'))

# VGG-16 Modified 2
# model = Sequential()
# model.add(Conv2D(64, kernel_size=(1, 1),activation='relu',input_shape=input_shape))
# model.add(BatchNormalization())
# model.add(Conv2D(64, (1, 1),activation='relu'))
# model.add(BatchNormalization())
# model.add(MaxPooling2D(pool_size=(2, 2)))
# model.add(Conv2D(64, kernel_size=(3, 3),activation='relu'))
# model.add(BatchNormalization())
# model.add(Conv2D(64, kernel_size=(3, 3),activation='relu'))
# model.add(BatchNormalization())
# model.add(MaxPooling2D(pool_size=(2, 2)))
# model.add(Conv2D(128, kernel_size=(3, 3),activation='relu'))
# model.add(BatchNormalization())
# model.add(Conv2D(128, kernel_size=(3, 3),activation='relu'))
# model.add(BatchNormalization())
# model.add(Conv2D(128, kernel_size=(3, 3),activation='relu'))
# model.add(BatchNormalization())
# model.add(MaxPooling2D(pool_size=(2, 2)))
# model.add(Conv2D(128, kernel_size=(3, 3),activation='relu'))
# model.add(BatchNormalization())
# model.add(Conv2D(128, kernel_size=(3, 3),activation='relu'))
# model.add(BatchNormalization())
# model.add(MaxPooling2D(pool_size=(2, 2)))
# model.add(Dropout(0.5))
# model.add(Flatten())
# model.add(Dense(1024,activation='relu'))
# model.add(Dropout(0.5))
# model.add(Dense(10,activation='softmax'))


model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])

model.summary()
lr_scheduler = LearningRateScheduler(lr_schedule)
#lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),cooldown=0,patience=4,min_lr=0.5e-9)
lr_reducer = ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.5)
early_stop = EarlyStopping(monitor='val_acc',patience=10,verbose=1)
callbacks = [lr_reducer, lr_scheduler, early_stop]


history = model.fit(x_train, y_train, batch_size=batch_size, epochs = epochs,callbacks=callbacks,validation_data = (x_val,y_val))
score = model.evaluate(x_val, y_val, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

# # graphing
# # list all data in history
# # history1 = history # lr = 0.0001
# # history6 = history # lr = 0.0005
# # history2 = history # lr = 0.001
# # history5 = history # lr = 0.005
# # history3 = history # lr = 0.01
# history7 = history # lr = 0.05
# # history4 = history # lr = 0.1

# print(history.history.keys())

# from sklearn.externals import joblib 
  
# # Save the model as a pickle in a file 
# joblib.dump(history7, 'history7.pkl')
# #joblib.dump(history2, 'history2.pkl')

# # graph 7 models
# lr_0001 = joblib.load('history1.pkl')
# lr_0005 = joblib.load('history6.pkl')
# lr_001 = joblib.load('history2.pkl')
# lr_005 = joblib.load('history5.pkl')
# lr_01 = joblib.load('history3.pkl')
# lr_05 = joblib.load('history7.pkl')
# lr_1 = joblib.load('history4.pkl')

# plt.plot(lr_0001.history['val_acc'],alpha=0.8)
# plt.plot(lr_0005.history['val_acc'],alpha=0.8)
# plt.plot(lr_001.history['val_acc'],alpha=0.8)
# plt.plot(lr_005.history['val_acc'],alpha=0.8)
# plt.plot(lr_01.history['val_acc'],alpha=0.8)
# plt.plot(lr_05.history['val_acc'],alpha=0.8)
# plt.plot(lr_1.history['val_acc'],alpha=0.8)

# plt.title('model validation accuracy')
# plt.ylabel('validation accuracy')
# plt.xlabel('epoch')
# plt.legend(['lr=0.0001','lr=0.0005','lr=0.001','lr=0.005','lr=0.01','lr=0.05','lr=0.1'], loc='right')
# plt.savefig('val_acc.png')

# # summarize history for accuracy
# plt.plot(history.history['acc'])
# plt.plot(history.history['val_acc'])
# plt.title('model accuracy')
# plt.ylabel('accuracy')
# plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='lower right')
# #plt.show()
# plt.savefig('acc.png')

# # summarize history for loss
# plt.plot(history.history['loss'])
# plt.plot(history.history['val_loss'])
# plt.title('model loss')
# plt.ylabel('loss')
# plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper right')
# #plt.show()
# plt.savefig('loss.png')

# # summarize history for learning rate
# plt.plot(history.history['lr'])
# plt.title('learning rate')
# plt.ylabel('value')
# plt.xlabel('epoch')
# plt.legend(['learning rate'], loc='upper right')
# #plt.show()
# plt.savefig('lr.png')

# from sklearn.externals import joblib 
  
# # Save the model as a pickle in a file 
# joblib.dump(model, 'four_layer_VGG_2.pkl') 
  
# # Load the model from the file 
# vgg_from_joblib = joblib.load('four_layer_VGG_2.pkl')  
  
# # Use the loaded model to make predictions 
# #knn_from_joblib.predict(X_test)

# (-1) % 26

# vgg_from_joblib

# test_images = pd.read_pickle('/content/drive/My Drive/Colab Notebooks/data/test_max_x')
# test_images = test_images.reshape(test_images.shape[0],128,128,1)
# test_images = test_images.astype('float32')
# test_images /= 255

# predicted = vgg_from_joblib.predict(test_images, verbose=0)
# prediction = []
# for arr in predicted:
#     prediction.append(np.argmax(arr))
# prediction = pd.DataFrame(prediction)

# df = pd.DataFrame()
# id_list = []
# for i in range(0, 10000):
#     id_list.append(i)
# df["Id"] = id_list
# df["Label"] = prediction
# result_csv = df.to_csv("result.csv",index=None, header=True)